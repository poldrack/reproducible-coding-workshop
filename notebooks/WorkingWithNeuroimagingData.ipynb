{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686e28ff-ff81-468f-bf1c-7ed605819e17",
   "metadata": {},
   "source": [
    "### Working with neuroimaging data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a743ab-92c8-41c5-a461-e2e5ab818aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import nilearn\n",
    "import nilearn.plotting\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from nilearn.image import smooth_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf460b-792a-4103-b593-29461b344410",
   "metadata": {},
   "source": [
    "\n",
    "#### Data setup\n",
    "\n",
    "For this exercise we will data from a study by Smeets et al. shared in [OpenNeuro](https://openneuro.org/datasets/ds000157/versions/00001).  The study is a blocked design in which subjects were shown pictures of food and non-food images. We will look at one run from one subject as an example.  We first need to download the relevant data files from OpenNeuro, obtaining them directly from Amazon Web Services using the `boto3` package.  These data are stored in [BIDS](http:/bids.neuroimaging.io) format, which makes it easy to identify which files we need for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1360d-e3fd-4ca6-bad9-965293286509",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = 'ds000157'\n",
    "fmriprep_dir = raw_dir + '-fmriprep'\n",
    "task = 'passiveimageviewing'\n",
    "run = '' # '_run-1'\n",
    "ses = '' #'_ses-test'\n",
    "sub = 'sub-01'\n",
    "space = '' # '_space-MNI152NLin2009cAsym_res-2'\n",
    "\n",
    "images = {\n",
    "        'mask': f\"{sub}/func/{sub}{ses}_task-{task}{run}{space}_desc-brain_mask.nii.gz\",\n",
    "        'bold': f\"{sub}/func/{sub}{ses}_task-{task}{run}{space}_desc-preproc_bold.nii.gz\",\n",
    "        'boldref': f\"{sub}/func/{sub}{ses}_task-{task}{run}{space}_boldref.nii.gz\",\n",
    "        'confounds': f\"{sub}/func/{sub}{ses}_task-{task}{run}{space}_desc-confounds_timeseries.tsv\"\n",
    "}\n",
    "\n",
    "images = {k: os.path.join(fmriprep_dir, v) for k, v in images.items()}\n",
    "# \n",
    "events = {'events': f\"sub-01/{ses.replace('_', '')}func/sub-01{ses}_task-{task}{run}_events.tsv\"}\n",
    "# f'task-{task}{run}_events.tsv'}\n",
    "\n",
    "events = {k: os.path.join(raw_dir, v) for k, v in events.items()}\n",
    "\n",
    "def get_data(files, s3_bucket='openneuro-derivatives'):\n",
    "\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    for label, file in files.items():\n",
    "        if os.path.exists(file):\n",
    "            print('using existing file:', file)\n",
    "            continue\n",
    "        outfile = file\n",
    "        if 'derivatives' in s3_bucket:\n",
    "            file = os.path.join('fmriprep', file)\n",
    "        if not os.path.exists(os.path.dirname(outfile)):\n",
    "            os.makedirs(os.path.dirname(outfile))\n",
    "        print(f'downloading {label}: {file} to {outfile}')\n",
    "        s3.download_file(s3_bucket, file, outfile)\n",
    "\n",
    "get_data(images)\n",
    "get_data(events, s3_bucket='openneuro.org')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7afbd-3770-4979-b5f9-5e1bd2506a24",
   "metadata": {},
   "source": [
    "### Displaying nifti images\n",
    "\n",
    "The `nilearn` packages provides a number of [plotting tools](https://nilearn.github.io/dev/plotting/index.html) for neuroimaging data. First we will plot the BOLD reference image using `nilearn.plotting.plot_img`.  There are many different options, but by default it plots three orthogonal sections through the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d467508-db27-4064-84ed-be6f9d4bec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nilearn.plotting.plot_img(images['boldref'], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72d4bf-8ccc-4c62-a7f3-434774769ff9",
   "metadata": {},
   "source": [
    "### Loading data from nifti images\n",
    "\n",
    "In many cases we would like to load the contents of a NIFTI image for further analysis.  We can do this using the `nibabel` package.  First, we can load the image and look at the information in the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b9bf1-92f1-43d6-92b0-c671dc58c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(images['bold'])\n",
    "\n",
    "print(img.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a906800-53eb-415c-bb6f-27404594b2d8",
   "metadata": {},
   "source": [
    "There are two ways that we can access the data within the image object.  First, we can access them through via the `dataobj` property, which provides an `array proxy` that points to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea3264-c829-431a-a401-bf1c532b173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dataobj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c35a6-c83f-488b-89fd-993f9c7a5866",
   "metadata": {},
   "source": [
    "In general it is prefered to load the data into a new variable, using the `get_fdata()` method of the image object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8c247-827e-482f-aa70-8a29dc6a15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = img.get_fdata()\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dfdc70-dc4c-4558-8d7b-48dd803c02f3",
   "metadata": {},
   "source": [
    "Now we can work with the data as we would with any Numpy array. For example, let's plot the timecourse of one voxel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd758e72-5d12-4073-9cee-04e73423b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a timeseries from one voxel\n",
    "\n",
    "tr = img.header.get_zooms()[3]\n",
    "\n",
    "imgtimes = np.arange(0, img.shape[3] * tr, tr)\n",
    "plt.plot(imgtimes, data[36, 14, 6, :])\n",
    "plt.ylabel('BOLD signal')\n",
    "plt.xlabel('seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0c228-c906-4732-95b1-be8bc98c76e6",
   "metadata": {},
   "source": [
    "### loading data from a set of voxels\n",
    "\n",
    "For many analyses, we would prefer to load a 2-dimensional matrix, with a subset of voxels on one axis and timepoints on the other axis. For example, we might want to run an analysis only on voxels that are within the brain mask.  We can extract data from a set of mask voxels using nilearn's `NiftiMasker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797d789-0254-416e-a7d8-ede3b06b9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = nilearn.maskers.NiftiMasker(images['mask'], standardize=True)\n",
    "maskdata = masker.fit_transform(images['bold'])\n",
    "\n",
    "print(maskdata.shape)\n",
    "\n",
    "# confirm that the number of columns matches number of nonzero voxels in the brain mask\n",
    "\n",
    "assert maskdata.shape[1] == np.sum(nib.load(images['mask']).dataobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7be9e-87c5-4d26-80a7-91076552cdf0",
   "metadata": {},
   "source": [
    "#### Create a \"carpet plot\"\n",
    "\n",
    "A \"carpet plot\" is a two-dimensional plot that presents voxel intensities as an image, with voxels on the Y axis and timepoints on the X axis ([Power, 2017](https://www.sciencedirect.com/science/article/abs/pii/S1053811916303871)).  They are a very useful way to visualize potential problems with an fMRI dataset.  Here we present a carpet plot for the fMRI dataset loaded above, along with a plot of mean global fMRI signal at each timepoint and framewise displacement (a measure of head motion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3e0be-b69d-441c-8cee-4696782dfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1, 1]}, sharex=True)\n",
    "\n",
    "# transpose the maskdata since we want timepoints on the X axis\n",
    "ax[0].imshow(maskdata.T, aspect='auto', cmap='gray')\n",
    "plt.tight_layout()\n",
    "ax[0].set_ylabel('voxels')\n",
    "_ = ax[0].set_xlabel('timepoints')\n",
    "\n",
    "confound_df = pd.read_csv(images['confounds'], sep='\\t')\n",
    "ax[1].plot(confound_df.global_signal)\n",
    "ax[1].set_ylabel('global mean signal')\n",
    "\n",
    "ax[2].plot(confound_df.framewise_displacement)\n",
    "ax[2].set_ylabel('framewise displacement')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195bc1d7-2cda-47ed-baf8-6a8efe8978d5",
   "metadata": {},
   "source": [
    "Here we can see that head motion is sometimes associated with large whole-brain fluctuations in global signal, as described by Power and colleagues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968dc14-9e43-4a83-b1db-e83a5c31b51c",
   "metadata": {},
   "source": [
    "### Fitting a linear model to the data\n",
    "\n",
    "In task fMRI we generally fit a linear model (based on the task, along with a set of confound regressors) to identify voxels that show a significant association with the task. The `nilearn` package has a set of functions for performing linear modeling analyses on fMRI data, which provide all of the functionality needed to analyze an fMRI dataset.  Here we provide a simple example by fitting the model to our example dataset from above.  To build the model, we need to load the file that specifies when the events happened during the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd1cc1-d446-4740-9837-a3b8e2999ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set up the events file\n",
    "\n",
    "events_df = pd.read_csv(events['events'], sep='\\t')\n",
    "\n",
    "# the nilearn first level analysis tool requires a \"trial_type\" column\n",
    "if 'trial_type' not in events_df.columns:\n",
    "    events_df['trial_type'] = task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2e2b3-a5a5-43c9-bedc-43f585e2e181",
   "metadata": {},
   "source": [
    "Now we set up and estimate the model using the nilearn modeling tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52e784-bf7b-46fd-9b2b-01973dad9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nilearn.glm.first_level.FirstLevelModel(t_r = tr,  smoothing_fwhm=5,\n",
    "                                               mask_img=images['mask'],\n",
    "                                               minimize_memory=False)\n",
    "\n",
    "# include 24 motion parameters in model as confounds - must replace NaNs in first row with 0\n",
    "motion_params = confound_df[[i for i in confound_df.columns if 'trans_' in i or 'rot_' in i]].fillna(0)\n",
    "\n",
    "modelfit = model.fit(img, events_df[['onset', 'duration', 'trial_type']],\n",
    "                     confounds=motion_params) \n",
    "\n",
    "# extract the fitted response image\n",
    "fitted_response = modelfit.predicted[0].get_fdata()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944af6f-67ef-4dd9-808a-bf044aae51a0",
   "metadata": {},
   "source": [
    "Having fit the model, we need to define a contrast in order to create the relevant statistical maps.  Here we will define a simple contrast that compares activity for both food and non-food images against a resting baseline.  The `generate_report()` method creates a report that provides various information about the contrast result. Here we correct for multiple comparisons using the false discovery rate (FDR) correction; this correction is generally not optimal for images ([Chumbley & Friston, 2009](https://pubmed.ncbi.nlm.nih.gov/18603449/)) but we use it here for convenience. We also impose a cluster size threshold of 30 voxels to remove small clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675d63a-b74c-43d5-b14c-92293930dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conmtx = np.zeros(model.design_matrices_[0].shape[1])\n",
    "conmtx[1:3] = 1  # set both food and nonfood to 1\n",
    "\n",
    "modelfit.generate_report(conmtx, bg_img=images['boldref'],\n",
    "                        cluster_threshold=30, height_control='fdr', alpha=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509eb6c-bdc6-46b1-ad18-79b22d804809",
   "metadata": {},
   "source": [
    "In some cases we might want to work directly with the statistical images, which we can do by extracting them using the `compute_contrast()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a5258-e235-405b-bb57-be39eeb79125",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_map = model.compute_contrast(conmtx, output_type='z_score')\n",
    "\n",
    "_, z_threshold = threshold_stats_img(contrast_map, alpha=.01, height_control='fdr')\n",
    "print('False Discovery rate = 0.05 threshold: %.3f' % z_threshold)\n",
    "\n",
    "contrast_map_thresh = nilearn.image.threshold_img(contrast_map, threshold=z_threshold,\n",
    "                                                  cluster_threshold=30, two_sided=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d244908-a22c-4e33-8ba2-7802d88f4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "nilearn.plotting.plot_stat_map(contrast_map_thresh, threshold=z_threshold,\n",
    "                               bg_img=images['boldref'], \n",
    "                               display_mode='z', cut_coords=np.arange(-10, 30, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b8e8c-b1e6-47ca-8fe9-c2e753a723bb",
   "metadata": {},
   "source": [
    "We can also compare the food and non-food conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8efc6-8450-444f-a3be-fd093739579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conmtx = np.zeros(model.design_matrices_[0].shape[1])\n",
    "conmtx[1:3] = [1, -1]\n",
    "\n",
    "modelfit.generate_report(conmtx, bg_img=images['boldref'],\n",
    "                        cluster_threshold=30, height_control='fdr', alpha=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caea94-585b-4339-b4e4-cc22d15d0e5d",
   "metadata": {},
   "source": [
    "### Moving between voxel coordinates and spatial coordinates\n",
    "\n",
    "There are two ways to refer to particular voxels in an image.  First, we can refer to their index along each of the dimensions of the image; for example, `[3, 5, 8]` would refer to the fourth voxel along the X axis (because indexing starts at zero), fifth voxel along the Y axis, and 8th voxel along the Z axis.  However, we can also refer to them in spatial coordinates, in which the location refers to the distance along each dimension from the *origin* of the image. In data that have been normalized to a standard space such as MNI space, this would refer to the origin (i.e. [0, 0, 0]) in that space; in unnormalized images the origin is usually the center of the image. \n",
    "\n",
    "The NIFTI header contains a matrix (known as the *affine* matrix, obtained using the `affine` property) that defines the relationship between voxel coordinates and spatial coordinates.  The affine matrix provides a way to translate between voxel and spatial coordinates by matrix multiplication; see [here](https://nipy.org/nibabel/coordinate_systems.html) for more detail on the use of affine matrices and homogenous coordinates in neuroimaging.  In short, the first three elements in the diagonal of the affine matrix contain the voxel sizes that allow scaling of the coordinates, the first three elements in the fourth column define the origin which specifies the translation of the coordinates, and the off-diagonal elements in the top 3 X 3 matrix define the rotation of the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab3987-e1b8-41b1-bfde-2f3678903e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a47c09-2f41-4f94-90c3-b024d616b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzcoords = [36, 14, 6, 1]\n",
    "print('voxel coords:', xyzcoords)\n",
    "\n",
    "# to convert from voxel coords to spatial coords, use dot product of sform with voxel coords\n",
    "spatialcoords = img.affine.dot(np.array(xyzcoords))\n",
    "print('spatial coords:', spatialcoords)\n",
    "\n",
    "# to convert back from spatial coords to voxel coords, use dot product of inverse sform with spatial coords\n",
    "reconverted = np.linalg.inv(img.affine).dot(spatialcoords)\n",
    "print('converted back to voxel coords:', reconverted)\n",
    "\n",
    "# use an assertion test to ensure that this worked\n",
    "assert np.allclose(xyzcoords, reconverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d315aa-41fc-4d69-9dc5-39c66ab3c0c8",
   "metadata": {},
   "source": [
    "We can use this knoweldge to extract the data from a particular coordinate and plot it against its fitted response from the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27964-1f1c-4e4e-a458-76a1c3bb6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,6))\n",
    "\n",
    "# nilearn expects spatial coordinates for its cut_coords argument\n",
    "nilearn.plotting.plot_stat_map(contrast_map_thresh,  threshold=z_threshold,\n",
    "                               bg_img=images['boldref'],  display_mode='ortho', axes=ax[0],\n",
    "                              cut_coords = spatialcoords[:3])\n",
    "\n",
    "# to extract the data, we need the voxel coords\n",
    "voxelts = data[xyzcoords[0], xyzcoords[1], xyzcoords[2],  :]\n",
    "voxelts = voxelts - np.mean(voxelts)\n",
    "fittedts = fitted_response[xyzcoords[0], xyzcoords[1], xyzcoords[2],  :]\n",
    "\n",
    "print(f'r-squared = {np.corrcoef(voxelts, fittedts)[0, 1] ** 2}')\n",
    "ax[1].plot(imgtimes, voxelts)\n",
    "hrfscale = 100 # scale for visualization\n",
    "ax[1].plot(imgtimes,  fittedts * hrfscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e57fc8-e153-4fc3-93ed-d0c0d5154959",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In this exercise you will first create a spatially smoothed and denoised version of the fMRI data used above. You will then perform principal component analysis on the data, extracting the top 10 components. Next, you will plot the component maps alongside the timeseries of each component, and compute the correlation between the component timeseries and the task regressor (combining food and non-food blocks).\n",
    "\n",
    "First, create a smoothed version of the BOLD image using nilearn's [`smooth_img`](https://nilearn.github.io/stable/modules/generated/nilearn.image.smooth_img.html) function with a 5mm FWHM. Then, use the nifti masker to transform this into a 2D matrix (timepoints X voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedec082-5f6e-43e2-8e53-65e2027d1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwhm = 5\n",
    "smoothed_img = ...\n",
    "\n",
    "maskdata_smoothed = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5b93a-358d-45f2-8ef4-ec2ced24beff",
   "metadata": {},
   "source": [
    "Next, you will denoise the data, using the confounds included in the model above. To do this, you first need to create a confound design matrix, by removing the three task conditions (`break`, `food`, and `nonfood`) from the original design matrix. The original design matrix is found (as a pandas data frame) in `modelfit.design_matrices_[0]`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f0205-47db-40a2-87de-46b00844e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_model = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e8c63-42f9-4800-a2e5-f29a7ec10876",
   "metadata": {},
   "source": [
    "We also need to create the task regressor that we will use later, by summing over the `food` and `nonfood` conditions in the original design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e58f32-2244-4e51-b572-86de5258cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_regressor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a8e2-6a28-4606-89de-768ec6f24820",
   "metadata": {},
   "source": [
    "Now we will use the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class from scikit-learn to fit the confound model to the timeseries from each voxel, and then compute the residual from the model (which is the denoised data) by subtracting the predicted response from the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1bb45-e859-4ccd-a77e-741e9738efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model object\n",
    "lr = ...\n",
    "\n",
    "# fit the model\n",
    "...\n",
    "\n",
    "maskdata_smoothed_denoised = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71667308-12b7-44b8-97b3-f665070698e9",
   "metadata": {},
   "source": [
    "We will now use the `PCA` function from scikit-learn to perform principal component analysis on the denoised data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0947116-83b0-49bd-bf24-c9a4e5214fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ...\n",
    "\n",
    "prcomps = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b11d5-2aa4-4cd8-ac2d-dc3a7f9bb1c0",
   "metadata": {},
   "source": [
    "Now loop through the ten components and plot the following for each:\n",
    "\n",
    "- in the leftward column, use [`nilearn.plotting.plot_stat_map`](https://nilearn.github.io/stable/modules/generated/nilearn.plotting.plot_stat_map.html) to plot the principal component map. This first requires mapping the component values back into an image using [`masker.inverse_transform`](https://nilearn.github.io/stable/modules/generated/nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker.inverse_transform). \n",
    "- in the rightward column, plot the PCA component timeseries.  In the title of the plot, print the correlation between the component timeseries and the task regressor created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c1f1e-9f2d-4486-a46e-2c9b0a6e3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the plotting frame\n",
    "fig, ax = plt.subplots(10, 2, figsize=(20,16))\n",
    "\n",
    "# loop over components\n",
    "for i in range(...):\n",
    "    # create a component image from the component vector\n",
    "    comp_img = ...\n",
    "    # compute the correlation between the component timeseries and the task regressor\n",
    "    task_corr = ...\n",
    "    # plot the statistical map \n",
    "    \n",
    "    nilearn.plotting.plot_stat_map(..., bg_img=images['boldref'], \n",
    "                                   axes=ax[i][0], threshold=1,\n",
    "                                  display_mode='z', cut_coords=np.arange(-10,60, 15),\n",
    "                                  title=f'r(task) = {task_corr:0.2f}')\n",
    "    # plot the component timeseries in the second column\n",
    "    ax[i][1].plot(...)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b01f2a438bb2f1c73bb4594d7b96aade1401a76b920d2c9503f364300577c65b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
